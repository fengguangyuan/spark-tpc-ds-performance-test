{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Spark SQL and its performance using TPC-DS workload\n",
    "This notebook sets up the Spark environment to run TPC-DS bench-mark on 1GB scale factor. TPC-DS is a widely used industry standard decision support benchmark that is used to evaluate performance of the data processing engines. Given TPC-DS excercises some key data warehouse features, running TPC-DS successfully reflects the readiness of Spark in terms of addressing the need of a data warehouse application. Apache Spark v2.0 supports all 99 decision support queries that is part of this benchmark. \n",
    "\n",
    "This notebook is written in scala and is intended to help the spark developers gain understanding on the setup steps required to run the benchmark.\n",
    "<b>Please note :</b> Several additional tuning steps may be required when adapting this to an actual spark production cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the journey data\n",
    "- Clone the tpcds journey repository to get access to all the data and scripts that are required to excercise this journey. \n",
    "- Normally the data and queries are generated by running the data and query generation utility from the tpcds toolkit available at http://www.tpc.org/tpcds. However for ease of use, the data and queries are pre-generated for 1GB scale factor. \n",
    "- We use the pre-generated data and queries to demonstrate how they can be used to run the tpcds queries against spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup variables.\n",
    "* Sets up variables that are used in the rest of this notebook.\n",
    "* The path variables are relative to the git clone directory.\n",
    "* tpcdsDatabaseName is hard-coded to \"TPCDS1G\". This can be changed if a different database name is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPCDS root directory is at : /home/mlp/notebooks/spark-tpc-ds-performance-test\n",
      "TPCDS ddl scripts directory is at: /home/mlp/notebooks/spark-tpc-ds-performance-test/src/ddl/individual\n",
      "TPCDS data directory is at: /home/mlp/notebooks/spark-tpc-ds-performance-test/src/data\n",
      "TPCDS queries directory is at: /home/mlp/notebooks/spark-tpc-ds-performance-test/src/queries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tpcdsRootDir = /home/mlp/notebooks/spark-tpc-ds-performance-test\n",
       "tpcdsWorkDir = hdfs:///muggle.feng/spark-tpc-ds-performance-test/work\n",
       "tpcdsDdlDir = /home/mlp/notebooks/spark-tpc-ds-performance-test/src/ddl/individual\n",
       "tpcdsGenDataDir = /home/mlp/notebooks/spark-tpc-ds-performance-test/src/data\n",
       "tpcdsQueriesDir = /home/mlp/notebooks/spark-tpc-ds-performance-test/src/queries\n",
       "tpcdsDatabaseName = TPCDS\n",
       "totalTime = 0\n",
       "spark = org.apache.spark.sql.SparkSession@3183324b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@3183324b"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tpcdsRootDir = \"/home/mlp/notebooks/spark-tpc-ds-performance-test\"\n",
    "val tpcdsWorkDir = \"hdfs:///muggle.feng/spark-tpc-ds-performance-test/work\"\n",
    "val tpcdsDdlDir = s\"${tpcdsRootDir}/src/ddl/individual\"\n",
    "val tpcdsGenDataDir = s\"${tpcdsRootDir}/src/data\"\n",
    "val tpcdsQueriesDir = s\"${tpcdsRootDir}/src/queries\"\n",
    "val tpcdsDatabaseName = \"TPCDS\"\n",
    "var totalTime: Long = 0\n",
    "println(\"TPCDS root directory is at : \"+ tpcdsRootDir)\n",
    "println(\"TPCDS ddl scripts directory is at: \" + tpcdsDdlDir)\n",
    "println(\"TPCDS data directory is at: \"+ tpcdsGenDataDir)\n",
    "println(\"TPCDS queries directory is at: \"+ tpcdsQueriesDir)\n",
    "val spark = SparkSession.\n",
    "    builder().\n",
    "    appName(\"tpc-ds-performance-muggle-feng\").\n",
    "    config(\"spark.ui.showConsoleProgress\", false).\n",
    "    config(\"spark.sql.autoBroadcastJoinThreshold\", -1).\n",
    "    config(\"spark.sql.crossJoin.enabled\", true).\n",
    "    getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function definitions.\n",
    "* Defines the utility functions that are called from the cells below in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:75: error: not found: value sqlStmts\n",
       "         sqlStmts.map(stmt => spark.sql(stmt))\n",
       "         ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clearTableDirectory(tableName: String): Unit = {\n",
    "    import sys.process._\n",
    "    val commandStr1 = s\"rm -rf spark-warehouse/tpcds2g.db/${tableName}/*\"\n",
    "    val commandStr2 = s\"rm -rf spark-warehouse/tpcds2g.db/${tableName}\"\n",
    "    var exitCode = Process(commandStr1).!\n",
    "    exitCode = Process(commandStr2).!\n",
    "}\n",
    "\n",
    "def createDatabase(): Unit = {\n",
    "    spark.sql(s\"DROP DATABASE IF EXISTS ${tpcdsDatabaseName} CASCADE\")\n",
    "    spark.sql(s\"CREATE DATABASE ${tpcdsDatabaseName}\")\n",
    "    spark.sql(s\"USE ${tpcdsDatabaseName}\")\n",
    "}\n",
    "\n",
    "/**\n",
    " * Function to create a table in spark. It reads the DDL script for each of the\n",
    " * tpc-ds table and executes it on Spark.\n",
    " */\n",
    "def createTable(tableName: String): Unit = {\n",
    "  println(s\"Creating table $tableName ..\")\n",
    "  spark.sql(s\"DROP TABLE IF EXISTS $tableName\")\n",
    "  clearTableDirectory(tableName)  \n",
    "  val (fileName, content) = \n",
    "    spark.sparkContext.wholeTextFiles(s\"${tpcdsDdlDir}/$tableName.sql\").collect()(0) \n",
    "    \n",
    "  // Remove the replace for the .dat once it is fixed in the github repo\n",
    "  //val sqlStmts = content.stripLineEnd\n",
    "  //  .replace('\\n', ' ')\n",
    "  //  .replace(\"${TPCDS_GENDATA_DIR}\", tpcdsGenDataDir)\n",
    "  //  .replace(\"csv\", \"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").split(\";\")\n",
    "  sqlStmts.map(stmt => spark.sql(stmt))    \n",
    "}  \n",
    "\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import scala.io.Source\n",
    "\n",
    "def readFromLocal(fileName: String): String = {\n",
    "  val source = Source.fromFile(fileName)\n",
    "  return source.mkString\n",
    "}\n",
    "\n",
    "def runQuery(queryStr: String,\n",
    "             individual: Boolean = true,\n",
    "             resultDir: String): Seq[(String, Double, Int, String)] = {\n",
    "  val querySummary = ArrayBuffer.empty[(String, Double, Int, String)]\n",
    "  val queryName = s\"${tpcdsQueriesDir}/query${queryStr}.sql\"\n",
    "  //val (_, content) = spark.sparkContext.wholeTextFiles(queryName).collect()(0)\n",
    "  val content = readFromLocal(queryName)\n",
    "  val queries = content.split(\"\\n\")\n",
    "    .filterNot (_.startsWith(\"--\"))\n",
    "    .mkString(\" \").split(\";\")\n",
    "  \n",
    "  var cnt = 1  \n",
    "  for (query <- queries)  {\n",
    "   val start = System.nanoTime()\n",
    "   val df = spark.sql(query)   \n",
    "   val result = spark.sql(query).collect\n",
    "   val timeElapsed = (System.nanoTime() - start) / 1000000000\n",
    "   val name = if (queries.length > 1) {\n",
    "       s\"query${queryStr}-${cnt}\"\n",
    "   } else {\n",
    "       s\"query${queryStr}\"\n",
    "   }  \n",
    "   val resultFile = s\"${resultDir}/${name}-notebook.res\"  \n",
    "   df.coalesce(1)\n",
    "      .write.format(\"com.databricks.spark.csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .mode(\"overwrite\")\n",
    "      .save(resultFile)\n",
    "   totalTime = totalTime + timeElapsed\n",
    "  \n",
    "   querySummary += Tuple4.apply(name, timeElapsed, result.length, resultFile)\n",
    "   cnt += 1                \n",
    "  }\n",
    "  querySummary \n",
    "}\n",
    "\n",
    "// run function for each table in tables array\n",
    "def forEachTable(tables: Array[String], f: (String) => Unit): Unit = {\n",
    "  for ( table <- tables) {\n",
    "    try {\n",
    "      f(table)\n",
    "    } catch {\n",
    "      case e: Throwable => {\n",
    "        println(\"EXCEPTION!! \" + e.getMessage())\n",
    "        throw e\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "def runIndividualQuery(queryNum: Int, resultDir: String = tpcdsWorkDir ): DataFrame = {\n",
    "    val queryStr = \"%02d\".format(queryNum) \n",
    "    val testSummary = ArrayBuffer.empty[(String, Double, Int, String)] \n",
    "    try {      \n",
    "      println(s\"Running TPC-DS Query : $queryStr\")  \n",
    "      testSummary ++= runQuery(queryStr, true, resultDir)\n",
    "    } catch {\n",
    "        case e: Throwable => {\n",
    "            println(\"Error in query \"+ queryNum + \" msg = \" + e.getMessage)\n",
    "        }\n",
    "    }\n",
    "    testSummary.toDF(\"QueryName\",\"ElapsedTime\",\"RowsReturned\", \"ResultLocation\")\n",
    "}\n",
    "\n",
    "def runAllQueries(resultDir: String = tpcdsWorkDir): DataFrame = {\n",
    "  val testSummary = ArrayBuffer.empty[(String, Double, Int, String)]    \n",
    "  var queryErrors = 0\n",
    "  for (i <- 1 to 99) {\n",
    "    try{\n",
    "      val queryStr = \"%02d\".format(i)\n",
    "      println(s\"Running TPC-DS Query : $queryStr\")   \n",
    "      testSummary ++= runQuery(queryStr, false, resultDir)\n",
    "    } catch {\n",
    "       case e: Throwable => {\n",
    "            println(\"Error in query \"+ i + \" msg = \" + e.getMessage)\n",
    "            queryErrors += 1\n",
    "       }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  println(\"=====================================================\")\n",
    "  if ( queryErrors > 0) {\n",
    "    println(s\"Query execution failed with $queryErrors errors\")\n",
    "  } else {\n",
    "    println(\"All TPC-DS queries ran successfully\")\n",
    "  }\n",
    "  println (s\"Total Elapsed Time so far: ${totalTime} seconds.\")\n",
    "  println(\"=====================================================\")\n",
    "  testSummary.toDF(\"QueryName\",\"ElapsedTime\",\"RowsReturned\", \"ResultLocation\")\n",
    "}\n",
    "\n",
    "def displaySummary(summaryDF: DataFrame): Unit = {\n",
    "    summaryDF.select(\"QueryName\", \"ElapsedTime\", \"RowsReturned\").show(10000)\n",
    "}\n",
    "\n",
    "def displayResult(queryNum: Int, summaryDF: DataFrame) = {\n",
    "   val queryStr = \"%02d\".format(queryNum)\n",
    "   // Find result files for this query number. For some queries there are\n",
    "   // multiple result files. \n",
    "   val files = summaryDF.where(s\"queryName like 'query${queryStr}%'\").select(\"ResultLocation\").collect()\n",
    "   for (file <- files) {\n",
    "       val fileName = file.getString(0)\n",
    "       val df = spark.read\n",
    "         .format(\"csv\")\n",
    "         .option(\"header\", \"true\") //reading the headers\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .option(\"mode\", \"DROPMALFORMED\")\n",
    "         .load(fileName)\n",
    "       val numRows:Int = df.count().toInt\n",
    "       df.show(numRows, truncate=false)\n",
    "   }\n",
    "}\n",
    "\n",
    "def explainQuery(queryNum: Int) = {\n",
    "  val queryStr = \"%02d\".format(queryNum)  \n",
    "  val queryName = s\"${tpcdsQueriesDir}/query${queryStr}.sql\"   \n",
    "  //val (_, content) = spark.sparkContext.wholeTextFiles(queryName).collect()(0)\n",
    "  val content = readFromLocal(queryName)\n",
    "  val queries = content.split(\"\\n\")\n",
    "    .filterNot (_.startsWith(\"--\"))\n",
    "    .mkString(\" \").split(\";\")\n",
    "    \n",
    "  for (query <- queries)  {    \n",
    "    spark.sql(query).explain(true) \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the TPC-DS schema\n",
    "* Create the database as specified by tpcdsDatabaseName\n",
    "* Create all the TPC-DS tables\n",
    "* Load the data into the tables in parquet format. Since the data generated by tpc-ds toolkit is in CSV format, we do the loading in multi steps.\n",
    "  * Step 1: we create tables in csv format by pointing the location to the generated data\n",
    "  * Step 2: we create parquet tables by using CTAS to convert text data into parquet format\n",
    "  * Step 3: we drop the text based tables as we longer need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hive history file=/tmp/mlp/hive_job_log_ba32da5b-0368-4978-94d0-fa239d937214_645930869.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tables = Array(call_center, catalog_sales, customer_demographics, income_band, promotion, store, time_dim, web_returns, catalog_page, customer, date_dim, inventory, reason, store_returns, warehouse, web_sales, catalog_returns, customer_address, household_demographics, item, ship_mode, store_sales, web_page, web_site)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TPC-DS table names.\n",
    "val tables = Array(\"call_center\", \"catalog_sales\",\n",
    "                   \"customer_demographics\", \"income_band\",\n",
    "                   \"promotion\", \"store\", \"time_dim\", \"web_returns\",\n",
    "                   \"catalog_page\", \"customer\", \"date_dim\",\n",
    "                   \"inventory\", \"reason\", \"store_returns\", \"warehouse\",\n",
    "                   \"web_sales\", \"catalog_returns\", \"customer_address\",\n",
    "                   \"household_demographics\", \"item\", \"ship_mode\", \"store_sales\",\n",
    "                   \"web_page\", \"web_site\" )\n",
    "\n",
    "// Create database\n",
    "createDatabase\n",
    "\n",
    "// Create table\n",
    "forEachTable(tables, table => createTable(table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify table creation and data loading.\n",
    "* Run a simple Spark SQL query to get the count of rows\n",
    "* Verify that the row counts are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Loaded and verified the table counts successfully\n",
      "=====================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rowCounts = Array(6, 1441548, 1920800, 20, 300, 12, 86400, 71763, 11718, 100000, 73049, 11745000, 35, 287514, 5, 719384, 144067, 50000, 7200, 18000, 20, 2880404, 60, 30)\n",
       "expectedCounts = Array(6, 1441548, 1920800, 20, 300, 12, 86400, 71763, 11718, 100000, 73049, 11745000, 35, 287514, 5, 719384, 144067, 50000, 7200, 18000, 20, 2880404, 60, 30)\n",
       "errorCount = 0\n",
       "zippedCountsWithIndex = Array(((6,6),0), ((1441548,1441548),1), ((1920800,1920800),2), ((20,20),3), ((300,300),4), ((12,12),5), ((86400,86400),6), ((71763,71763),7), ((11718,11718),8), ((100000,100000),9), ((73049,73049),10), ((11745000,11745000),11), ((35,35),12), ((287514,287514),13), ((5,5),14), ((719384,719384),15), ((144067,144067),16), ((50000,50000),17), ((7200,7200),18)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(((6,6),0), ((1441548,1441548),1), ((1920800,1920800),2), ((20,20),3), ((300,300),4), ((12,12),5), ((86400,86400),6), ((71763,71763),7), ((11718,11718),8), ((100000,100000),9), ((73049,73049),10), ((11745000,11745000),11), ((35,35),12), ((287514,287514),13), ((5,5),14), ((719384,719384),15), ((144067,144067),16), ((50000,50000),17), ((7200,7200),18)..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Run a count query and get the counts\n",
    "val rowCounts = tables.map { table =>\n",
    "    spark.table(table).count()\n",
    "}\n",
    "\n",
    "val expectedCounts = Array (\n",
    "    6, 1441548, 1920800, 20, 300, 12, 86400,\n",
    "    71763,  11718, 100000, 73049, 11745000, \n",
    "    35, 287514, 5, 719384, 144067, 50000, 7200,\n",
    "    18000, 20, 2880404, 60, 30\n",
    ")\n",
    "\n",
    "var errorCount = 0;\n",
    "val zippedCountsWithIndex = rowCounts.zip(expectedCounts).zipWithIndex\n",
    "for ((pair, index) <- zippedCountsWithIndex) {\n",
    "    if (pair._1 != pair._2) {\n",
    "        println(s\"\"\"ERROR!! Row counts for ${tables(index)} does not match.\n",
    "        Expected=${expectedCounts(index)} but found ${rowCounts(index)}\"\"\")\n",
    "        errorCount += 1\n",
    "    }\n",
    "}\n",
    "\n",
    "println(\"=====================================================\")\n",
    "if ( errorCount > 0) {\n",
    "  println(s\"Load verification failed with $errorCount errors\")\n",
    "} else {\n",
    "  println(\"Loaded and verified the table counts successfully\")\n",
    "}\n",
    "println(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a single query\n",
    "* Run a query given a query number between 1 to 99\n",
    "* Display the query results, the elapsed time to execute the query and the number of rows returned for the query\n",
    "* To run a different query, please change the QUERY_NUM to a valid value from 1 to 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TPC-DS Query : 01\n",
      "+---------+-----------+------------+\n",
      "|QueryName|ElapsedTime|RowsReturned|\n",
      "+---------+-----------+------------+\n",
      "|  query01|       27.0|         100|\n",
      "+---------+-----------+------------+\n",
      "\n",
      "+----------------+\n",
      "|c_customer_id   |\n",
      "+----------------+\n",
      "|AAAAAAAAAAABBAAA|\n",
      "|AAAAAAAAAAADBAAA|\n",
      "|AAAAAAAAAAADBAAA|\n",
      "|AAAAAAAAAAAKAAAA|\n",
      "|AAAAAAAAAABDAAAA|\n",
      "|AAAAAAAAAABHBAAA|\n",
      "|AAAAAAAAAABLAAAA|\n",
      "|AAAAAAAAAABMAAAA|\n",
      "|AAAAAAAAAACHAAAA|\n",
      "|AAAAAAAAAACMAAAA|\n",
      "|AAAAAAAAAADDAAAA|\n",
      "|AAAAAAAAAADGAAAA|\n",
      "|AAAAAAAAAADGBAAA|\n",
      "|AAAAAAAAAADGBAAA|\n",
      "|AAAAAAAAAADPAAAA|\n",
      "|AAAAAAAAAAEBAAAA|\n",
      "|AAAAAAAAAAEFBAAA|\n",
      "|AAAAAAAAAAEGBAAA|\n",
      "|AAAAAAAAAAEIAAAA|\n",
      "|AAAAAAAAAAEMAAAA|\n",
      "|AAAAAAAAAAFAAAAA|\n",
      "|AAAAAAAAAAFPAAAA|\n",
      "|AAAAAAAAAAGGBAAA|\n",
      "|AAAAAAAAAAGHBAAA|\n",
      "|AAAAAAAAAAGJAAAA|\n",
      "|AAAAAAAAAAGMAAAA|\n",
      "|AAAAAAAAAAHEBAAA|\n",
      "|AAAAAAAAAAHFBAAA|\n",
      "|AAAAAAAAAAIEBAAA|\n",
      "|AAAAAAAAAAJGBAAA|\n",
      "|AAAAAAAAAAJHBAAA|\n",
      "|AAAAAAAAAAKCAAAA|\n",
      "|AAAAAAAAAAKCAAAA|\n",
      "|AAAAAAAAAAKJAAAA|\n",
      "|AAAAAAAAAAKMAAAA|\n",
      "|AAAAAAAAAAKMAAAA|\n",
      "|AAAAAAAAAALAAAAA|\n",
      "|AAAAAAAAAALABAAA|\n",
      "|AAAAAAAAAALGAAAA|\n",
      "|AAAAAAAAAALHBAAA|\n",
      "|AAAAAAAAAALJAAAA|\n",
      "|AAAAAAAAAANHAAAA|\n",
      "|AAAAAAAAAANHBAAA|\n",
      "|AAAAAAAAAANJAAAA|\n",
      "|AAAAAAAAAANMAAAA|\n",
      "|AAAAAAAAAANMAAAA|\n",
      "|AAAAAAAAAANNAAAA|\n",
      "|AAAAAAAAAAOBBAAA|\n",
      "|AAAAAAAAAAODBAAA|\n",
      "|AAAAAAAAAAOLAAAA|\n",
      "|AAAAAAAAAAPGBAAA|\n",
      "|AAAAAAAAABAAAAAA|\n",
      "|AAAAAAAAABAEAAAA|\n",
      "|AAAAAAAAABAEBAAA|\n",
      "|AAAAAAAAABAFBAAA|\n",
      "|AAAAAAAAABAIAAAA|\n",
      "|AAAAAAAAABAOAAAA|\n",
      "|AAAAAAAAABBDBAAA|\n",
      "|AAAAAAAAABCFAAAA|\n",
      "|AAAAAAAAABCHBAAA|\n",
      "|AAAAAAAAABDHAAAA|\n",
      "|AAAAAAAAABENAAAA|\n",
      "|AAAAAAAAABFEBAAA|\n",
      "|AAAAAAAAABFGAAAA|\n",
      "|AAAAAAAAABFMAAAA|\n",
      "|AAAAAAAAABFPAAAA|\n",
      "|AAAAAAAAABGFAAAA|\n",
      "|AAAAAAAAABGFBAAA|\n",
      "|AAAAAAAAABGJAAAA|\n",
      "|AAAAAAAAABIBBAAA|\n",
      "|AAAAAAAAABICBAAA|\n",
      "|AAAAAAAAABIIAAAA|\n",
      "|AAAAAAAAABJNAAAA|\n",
      "|AAAAAAAAABKGBAAA|\n",
      "|AAAAAAAAABLOAAAA|\n",
      "|AAAAAAAAABLPAAAA|\n",
      "|AAAAAAAAABMABAAA|\n",
      "|AAAAAAAAABMPAAAA|\n",
      "|AAAAAAAAABNAAAAA|\n",
      "|AAAAAAAAABNCBAAA|\n",
      "|AAAAAAAAABNEBAAA|\n",
      "|AAAAAAAAABNLAAAA|\n",
      "|AAAAAAAAABNOAAAA|\n",
      "|AAAAAAAAABNPAAAA|\n",
      "|AAAAAAAAABOAAAAA|\n",
      "|AAAAAAAAABOFBAAA|\n",
      "|AAAAAAAAABOOAAAA|\n",
      "|AAAAAAAAABOPAAAA|\n",
      "|AAAAAAAAABPEAAAA|\n",
      "|AAAAAAAAACADAAAA|\n",
      "|AAAAAAAAACAFAAAA|\n",
      "|AAAAAAAAACAFAAAA|\n",
      "|AAAAAAAAACAHBAAA|\n",
      "|AAAAAAAAACAJAAAA|\n",
      "|AAAAAAAAACBDAAAA|\n",
      "|AAAAAAAAACBDAAAA|\n",
      "|AAAAAAAAACBEBAAA|\n",
      "|AAAAAAAAACBNAAAA|\n",
      "|AAAAAAAAACBPAAAA|\n",
      "|AAAAAAAAACCHAAAA|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QUERY_NUM = 1\n",
       "result = [QueryName: string, ElapsedTime: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[QueryName: string, ElapsedTime: double ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val QUERY_NUM = 1\n",
    "val result = runIndividualQuery(QUERY_NUM, \"hdfs:///muggle.feng\")\n",
    "displaySummary(result)\n",
    "displayResult(QUERY_NUM, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all the TPC-DS queries\n",
    "* Runs all the queries starting from 1 to 99\n",
    "* The query results are saved and can be queried by calling getResults method.\n",
    "* The summary will be shown at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TPC-DS Query : 01\n",
      "Running TPC-DS Query : 02\n",
      "Running TPC-DS Query : 03\n",
      "Running TPC-DS Query : 04\n",
      "Running TPC-DS Query : 05\n",
      "Running TPC-DS Query : 06\n",
      "Running TPC-DS Query : 07\n",
      "Running TPC-DS Query : 08\n",
      "Running TPC-DS Query : 09\n",
      "Running TPC-DS Query : 10\n",
      "Running TPC-DS Query : 11\n",
      "Running TPC-DS Query : 12\n",
      "Running TPC-DS Query : 13\n",
      "Running TPC-DS Query : 14\n",
      "Running TPC-DS Query : 15\n",
      "Running TPC-DS Query : 16\n",
      "Running TPC-DS Query : 17\n",
      "Running TPC-DS Query : 18\n",
      "Running TPC-DS Query : 19\n",
      "Running TPC-DS Query : 20\n",
      "Running TPC-DS Query : 21\n",
      "Running TPC-DS Query : 22\n",
      "Running TPC-DS Query : 23\n",
      "Running TPC-DS Query : 24\n",
      "Running TPC-DS Query : 25\n",
      "Running TPC-DS Query : 26\n",
      "Running TPC-DS Query : 27\n",
      "Running TPC-DS Query : 28\n",
      "Running TPC-DS Query : 29\n",
      "Running TPC-DS Query : 30\n",
      "Running TPC-DS Query : 31\n",
      "Running TPC-DS Query : 32\n",
      "Running TPC-DS Query : 33\n",
      "Running TPC-DS Query : 34\n",
      "Running TPC-DS Query : 35\n",
      "Running TPC-DS Query : 36\n",
      "Running TPC-DS Query : 37\n",
      "Running TPC-DS Query : 38\n",
      "Running TPC-DS Query : 39\n",
      "Running TPC-DS Query : 40\n",
      "Running TPC-DS Query : 41\n",
      "Running TPC-DS Query : 42\n",
      "Running TPC-DS Query : 43\n",
      "Running TPC-DS Query : 44\n",
      "Running TPC-DS Query : 45\n",
      "Running TPC-DS Query : 46\n",
      "Running TPC-DS Query : 47\n",
      "Running TPC-DS Query : 48\n",
      "Running TPC-DS Query : 49\n",
      "Running TPC-DS Query : 50\n",
      "Running TPC-DS Query : 51\n",
      "Running TPC-DS Query : 52\n",
      "Running TPC-DS Query : 53\n",
      "Running TPC-DS Query : 54\n",
      "Running TPC-DS Query : 55\n",
      "Running TPC-DS Query : 56\n",
      "Running TPC-DS Query : 57\n",
      "Running TPC-DS Query : 58\n",
      "Running TPC-DS Query : 59\n",
      "Running TPC-DS Query : 60\n",
      "Running TPC-DS Query : 61\n",
      "Running TPC-DS Query : 62\n",
      "Running TPC-DS Query : 63\n",
      "Running TPC-DS Query : 64\n",
      "Running TPC-DS Query : 65\n",
      "Running TPC-DS Query : 66\n",
      "Running TPC-DS Query : 67\n",
      "Running TPC-DS Query : 68\n",
      "Running TPC-DS Query : 69\n",
      "Running TPC-DS Query : 70\n",
      "Running TPC-DS Query : 71\n",
      "Running TPC-DS Query : 72\n",
      "Running TPC-DS Query : 73\n",
      "Running TPC-DS Query : 74\n",
      "Running TPC-DS Query : 75\n",
      "Running TPC-DS Query : 76\n",
      "Running TPC-DS Query : 77\n",
      "Running TPC-DS Query : 78\n",
      "Running TPC-DS Query : 79\n",
      "Running TPC-DS Query : 80\n",
      "Running TPC-DS Query : 81\n",
      "Running TPC-DS Query : 82\n",
      "Running TPC-DS Query : 83\n",
      "Running TPC-DS Query : 84\n",
      "Running TPC-DS Query : 85\n",
      "Running TPC-DS Query : 86\n",
      "Running TPC-DS Query : 87\n",
      "Running TPC-DS Query : 88\n",
      "Running TPC-DS Query : 89\n",
      "Running TPC-DS Query : 90\n",
      "Running TPC-DS Query : 91\n",
      "Running TPC-DS Query : 92\n",
      "Running TPC-DS Query : 93\n",
      "Running TPC-DS Query : 94\n",
      "Running TPC-DS Query : 95\n",
      "Running TPC-DS Query : 96\n",
      "Running TPC-DS Query : 97\n",
      "Running TPC-DS Query : 98\n",
      "Running TPC-DS Query : 99\n",
      "=====================================================\n",
      "All TPC-DS queries ran successfully\n",
      "Total Elapsed Time so far: 1905 seconds.\n",
      "=====================================================\n",
      "+---------+-----------+------------+\n",
      "|QueryName|ElapsedTime|RowsReturned|\n",
      "+---------+-----------+------------+\n",
      "|  query01|       10.0|         100|\n",
      "|  query02|        7.0|        2513|\n",
      "|  query03|        9.0|          89|\n",
      "|  query04|       36.0|           8|\n",
      "|  query05|       16.0|         100|\n",
      "|  query06|       11.0|          45|\n",
      "|  query07|        7.0|         100|\n",
      "|  query08|        7.0|           5|\n",
      "|  query09|        4.0|           1|\n",
      "|  query10|        7.0|           5|\n",
      "|  query11|       23.0|          88|\n",
      "|  query12|        6.0|         100|\n",
      "|  query13|        8.0|           1|\n",
      "|query14-1|       60.0|         100|\n",
      "|query14-2|       40.0|         100|\n",
      "|  query15|        6.0|         100|\n",
      "|  query16|        8.0|           1|\n",
      "|  query17|        9.0|           1|\n",
      "|  query18|        8.0|         100|\n",
      "|  query19|        8.0|         100|\n",
      "|  query20|        6.0|         100|\n",
      "|  query21|       12.0|         100|\n",
      "|  query22|        8.0|         100|\n",
      "|query23-1|       33.0|           1|\n",
      "|query23-2|       22.0|           4|\n",
      "|query24-1|       16.0|           0|\n",
      "|query24-2|       14.0|           0|\n",
      "|  query25|       10.0|           1|\n",
      "|  query26|        5.0|         100|\n",
      "|  query27|        5.0|         100|\n",
      "|  query28|        6.0|           1|\n",
      "|  query29|        9.0|           1|\n",
      "|  query30|       11.0|         100|\n",
      "|  query31|       17.0|          52|\n",
      "|  query32|        4.0|           1|\n",
      "|  query33|       13.0|         100|\n",
      "|  query34|        5.0|         451|\n",
      "|  query35|       11.0|         100|\n",
      "|  query36|        7.0|         100|\n",
      "|  query37|        3.0|           1|\n",
      "|  query38|       16.0|           1|\n",
      "|query39-1|       18.0|         246|\n",
      "|query39-2|       15.0|          17|\n",
      "|  query40|        7.0|         100|\n",
      "|  query41|        2.0|           4|\n",
      "|  query42|        3.0|          10|\n",
      "|  query43|        3.0|           6|\n",
      "|  query44|        4.0|          10|\n",
      "|  query45|        8.0|          19|\n",
      "|  query46|        7.0|         100|\n",
      "|  query47|       26.0|         100|\n",
      "|  query48|        6.0|           1|\n",
      "|  query49|        8.0|          32|\n",
      "|  query50|        7.0|           6|\n",
      "|  query51|       23.0|         100|\n",
      "|  query52|        3.0|         100|\n",
      "|  query53|        6.0|         100|\n",
      "|  query54|       10.0|           1|\n",
      "|  query55|        7.0|         100|\n",
      "|  query56|       33.0|         100|\n",
      "|  query57|       59.0|         100|\n",
      "|  query58|       56.0|           3|\n",
      "|  query59|       19.0|         100|\n",
      "|  query60|       40.0|         100|\n",
      "|  query61|       42.0|           1|\n",
      "|  query62|       14.0|         100|\n",
      "|  query63|       17.0|         100|\n",
      "|  query64|      189.0|          10|\n",
      "|  query65|       33.0|         100|\n",
      "|  query66|       43.0|           5|\n",
      "|  query67|       12.0|         100|\n",
      "|  query68|        6.0|         100|\n",
      "|  query69|        8.0|         100|\n",
      "|  query70|       10.0|           3|\n",
      "|  query71|        7.0|        1018|\n",
      "|  query72|       95.0|         100|\n",
      "|  query73|        4.0|           5|\n",
      "|  query74|       22.0|          92|\n",
      "|  query75|       45.0|         100|\n",
      "|  query76|        8.0|         100|\n",
      "|  query77|      120.0|          44|\n",
      "|  query78|       64.0|         100|\n",
      "|  query79|        6.0|         100|\n",
      "|  query80|       34.0|         100|\n",
      "|  query81|       24.0|         100|\n",
      "|  query82|        3.0|           2|\n",
      "|  query83|       21.0|          21|\n",
      "|  query84|        4.0|          25|\n",
      "|  query85|       11.0|           6|\n",
      "|  query86|        7.0|         100|\n",
      "|  query87|       23.0|           1|\n",
      "|  query88|       21.0|           1|\n",
      "|  query89|        8.0|         100|\n",
      "|  query90|        4.0|           1|\n",
      "|  query91|       10.0|           1|\n",
      "|  query92|        5.0|           1|\n",
      "|  query93|        4.0|         100|\n",
      "|  query94|       11.0|           1|\n",
      "|  query95|       10.0|           1|\n",
      "|  query96|        9.0|           1|\n",
      "|  query97|       13.0|           1|\n",
      "|  query98|       12.0|        2516|\n",
      "|  query99|        6.0|          90|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [QueryName: string, ElapsedTime: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[QueryName: string, ElapsedTime: double ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = runAllQueries()\n",
    "displaySummary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Result for a individual Query\n",
    "* Reads the result file for the given query stored when thery are run in previous steps.\n",
    "* Certain queries have multiple associated result files. The result files are read in sequence and\n",
    "  results are displayed.\n",
    "* If the result file(s) are not found , then an error is displayed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|c_customer_id   |\n",
      "+----------------+\n",
      "|AAAAAAAAAAABBAAA|\n",
      "|AAAAAAAAAAADBAAA|\n",
      "|AAAAAAAAAAADBAAA|\n",
      "|AAAAAAAAAAAKAAAA|\n",
      "|AAAAAAAAAABDAAAA|\n",
      "|AAAAAAAAAABHBAAA|\n",
      "|AAAAAAAAAABLAAAA|\n",
      "|AAAAAAAAAABMAAAA|\n",
      "|AAAAAAAAAACHAAAA|\n",
      "|AAAAAAAAAACMAAAA|\n",
      "|AAAAAAAAAADDAAAA|\n",
      "|AAAAAAAAAADGAAAA|\n",
      "|AAAAAAAAAADGBAAA|\n",
      "|AAAAAAAAAADGBAAA|\n",
      "|AAAAAAAAAADPAAAA|\n",
      "|AAAAAAAAAAEBAAAA|\n",
      "|AAAAAAAAAAEFBAAA|\n",
      "|AAAAAAAAAAEGBAAA|\n",
      "|AAAAAAAAAAEIAAAA|\n",
      "|AAAAAAAAAAEMAAAA|\n",
      "|AAAAAAAAAAFAAAAA|\n",
      "|AAAAAAAAAAFPAAAA|\n",
      "|AAAAAAAAAAGGBAAA|\n",
      "|AAAAAAAAAAGHBAAA|\n",
      "|AAAAAAAAAAGJAAAA|\n",
      "|AAAAAAAAAAGMAAAA|\n",
      "|AAAAAAAAAAHEBAAA|\n",
      "|AAAAAAAAAAHFBAAA|\n",
      "|AAAAAAAAAAIEBAAA|\n",
      "|AAAAAAAAAAJGBAAA|\n",
      "|AAAAAAAAAAJHBAAA|\n",
      "|AAAAAAAAAAKCAAAA|\n",
      "|AAAAAAAAAAKCAAAA|\n",
      "|AAAAAAAAAAKJAAAA|\n",
      "|AAAAAAAAAAKMAAAA|\n",
      "|AAAAAAAAAAKMAAAA|\n",
      "|AAAAAAAAAALAAAAA|\n",
      "|AAAAAAAAAALABAAA|\n",
      "|AAAAAAAAAALGAAAA|\n",
      "|AAAAAAAAAALHBAAA|\n",
      "|AAAAAAAAAALJAAAA|\n",
      "|AAAAAAAAAANHAAAA|\n",
      "|AAAAAAAAAANHBAAA|\n",
      "|AAAAAAAAAANJAAAA|\n",
      "|AAAAAAAAAANMAAAA|\n",
      "|AAAAAAAAAANMAAAA|\n",
      "|AAAAAAAAAANNAAAA|\n",
      "|AAAAAAAAAAOBBAAA|\n",
      "|AAAAAAAAAAODBAAA|\n",
      "|AAAAAAAAAAOLAAAA|\n",
      "|AAAAAAAAAAPGBAAA|\n",
      "|AAAAAAAAABAAAAAA|\n",
      "|AAAAAAAAABAEAAAA|\n",
      "|AAAAAAAAABAEBAAA|\n",
      "|AAAAAAAAABAFBAAA|\n",
      "|AAAAAAAAABAIAAAA|\n",
      "|AAAAAAAAABAOAAAA|\n",
      "|AAAAAAAAABBDBAAA|\n",
      "|AAAAAAAAABCFAAAA|\n",
      "|AAAAAAAAABCHBAAA|\n",
      "|AAAAAAAAABDHAAAA|\n",
      "|AAAAAAAAABENAAAA|\n",
      "|AAAAAAAAABFEBAAA|\n",
      "|AAAAAAAAABFGAAAA|\n",
      "|AAAAAAAAABFMAAAA|\n",
      "|AAAAAAAAABFPAAAA|\n",
      "|AAAAAAAAABGFAAAA|\n",
      "|AAAAAAAAABGFBAAA|\n",
      "|AAAAAAAAABGJAAAA|\n",
      "|AAAAAAAAABIBBAAA|\n",
      "|AAAAAAAAABICBAAA|\n",
      "|AAAAAAAAABIIAAAA|\n",
      "|AAAAAAAAABJNAAAA|\n",
      "|AAAAAAAAABKGBAAA|\n",
      "|AAAAAAAAABLOAAAA|\n",
      "|AAAAAAAAABLPAAAA|\n",
      "|AAAAAAAAABMABAAA|\n",
      "|AAAAAAAAABMPAAAA|\n",
      "|AAAAAAAAABNAAAAA|\n",
      "|AAAAAAAAABNCBAAA|\n",
      "|AAAAAAAAABNEBAAA|\n",
      "|AAAAAAAAABNLAAAA|\n",
      "|AAAAAAAAABNOAAAA|\n",
      "|AAAAAAAAABNPAAAA|\n",
      "|AAAAAAAAABOAAAAA|\n",
      "|AAAAAAAAABOFBAAA|\n",
      "|AAAAAAAAABOOAAAA|\n",
      "|AAAAAAAAABOPAAAA|\n",
      "|AAAAAAAAABPEAAAA|\n",
      "|AAAAAAAAACADAAAA|\n",
      "|AAAAAAAAACAFAAAA|\n",
      "|AAAAAAAAACAFAAAA|\n",
      "|AAAAAAAAACAHBAAA|\n",
      "|AAAAAAAAACAJAAAA|\n",
      "|AAAAAAAAACBDAAAA|\n",
      "|AAAAAAAAACBDAAAA|\n",
      "|AAAAAAAAACBEBAAA|\n",
      "|AAAAAAAAACBNAAAA|\n",
      "|AAAAAAAAACBPAAAA|\n",
      "|AAAAAAAAACCHAAAA|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "displayResult(1, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display SQL Execution Plan\n",
    "* Display the analyzed, optimized and phyical plan for a given query.\n",
    "* Can be used by developers for debugging purposes.\n",
    "* QUERY_NUM can be changed to display the plan for different query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapreduce.lib.input.InvalidInputException\n",
       "Message: Input path does not exist: hdfs://rtcluster/home/mlp/notebooks/spark-tpc-ds-performance-test/src/queries/query01.sql\n",
       "StackTrace:   at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n",
       "  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n",
       "  at org.apache.spark.input.WholeTextFileInputFormat.setMinPartitions(WholeTextFileInputFormat.scala:52)\n",
       "  at org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:54)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n",
       "  at explainQuery(<console>:192)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val QUERY_NUM=1\n",
    "explainQuery(QUERY_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Error parsing magics!\n",
       "Message: Magic brunel does not exist!\n",
       "StackTrace: "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%brunel\n",
    "data('result') bar x(QueryName) y(ElapsedTime) title(\"Query Execution Time in seconds\", \"Execution Summary\":footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "Visit [Apache Spark](https://spark.apache.org) for learning about spark. For questions or requests plese visit [Spark Community](https://spark.apache.org/community.html). To get involved , see [Contributing to Apache Spark](https://spark.apache.org/contributing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "* Dilip Biswal is a Senior Software Engineer at the Spark Technology Center at IBM. He is an active Apache Spark contributor and works in the open source community.\n",
    "  He is experienced in Relational Databases, Distributed Computing and Big Data Analytics.  He has extensively worked on SQL engines like Informix, Derby, and Big SQL.\n",
    "* Sunitha Kambhampati is an Advisory Software Engineer at the Spark Technology Center at IBM. She is an Apache Spark contributor and works in the open source community. She is experienced in Big Data Analytics.\n",
    "* Xin Wu is an Advisory Software Engineer and is an active contributor for Apache Spark. He has experiences in distributed query processing engines like BigSQL, DB2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
